{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cripser in c:\\users\\91703\\anaconda3\\lib\\site-packages (0.0.13)\n"
     ]
    }
   ],
   "source": [
    "! pip install cripser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27564\\4223269485.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Compute PH for the V-construction of the original image (pixel value filtration)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpd_v_construction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputePH\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg3d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mbetti_numbers_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd_v_construction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpd_v_construction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Betti numbers (V-construction): \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetti_numbers_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cr' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import persim\n",
    "\n",
    "# Replace this with your code to compute persistence diagrams for the V-construction\n",
    "# Compute PH for the V-construction of the original image (pixel value filtration)\n",
    "start = time.time()\n",
    "pd_v_construction = cr.computePH(img3d)\n",
    "betti_numbers_v = [len(pd_v_construction[pd_v_construction[:,0] == i]) for i in range(3)]\n",
    "print(\"Betti numbers (V-construction): \", betti_numbers_v)\n",
    "print(\"elapsed_time:{} sec\".format(time.time() - start))\n",
    "\n",
    "# Replace this with your code to compute persistence diagrams for the T-construction\n",
    "# Compute PH for the T-construction of the original image (pixel value filtration)\n",
    "start = time.time()\n",
    "pd_t_construction = tcr.computePH(img3d)\n",
    "betti_numbers_t = [len(pd_t_construction[pd_t_construction[:,0] == i]) for i in range(3)]\n",
    "print(\"Betti numbers (T-construction): \", betti_numbers_t)\n",
    "print(\"elapsed_time:{} sec\".format(time.time() - start))\n",
    "\n",
    "# Plot persistent diagrams using persim\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "persim.plot_diagrams([p[:,1:3] for p in pd_v_construction], ax=axs[0], title='V-construction')\n",
    "persim.plot_diagrams([p[:,1:3] for p in pd_t_construction], ax=axs[1], title='T-construction')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27564\\2663277522.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# Load images from folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mimg_stack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_images_from_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m# Compute persistence image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27564\\2663277522.py\u001b[0m in \u001b[0;36mload_images_from_folder\u001b[1;34m(folder)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mimg_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrgb2gray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_gray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Stack images along the last axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91703\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\91703\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all input arrays must have the same shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import threshold_otsu\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            # Convert to grayscale, handling RGBA images\n",
    "            if len(img.shape) == 3 and img.shape[2] == 4:\n",
    "                img_gray = rgb2gray(img[:, :, :3])  # Extract RGB channels\n",
    "            else:\n",
    "                img_gray = rgb2gray(img)\n",
    "            images.append(img_gray)\n",
    "    return np.stack(images, axis=-1)  # Stack images along the last axis\n",
    "\n",
    "\n",
    "def compute_persistence_image(img_stack):\n",
    "    # Apply distance transform\n",
    "    dt_img_stack = np.zeros_like(img_stack, dtype=np.float64)  \n",
    "    for i in range(img_stack.shape[-1]):\n",
    "        dt_img_stack[:,:,i] = distance_transform_edt(img_stack[:,:,i])\n",
    "        inverted_img = ~img_stack[:,:,i].astype(bool)\n",
    "        dt_inverted_img = distance_transform_edt(inverted_img).astype(np.float64)  \n",
    "        dt_img_stack[:,:,i] -= dt_inverted_img  \n",
    "\n",
    "    # Compute pairwise distances\n",
    "    pairwise_distances = euclidean_distances(dt_img_stack.reshape((-1, dt_img_stack.shape[-1])))\n",
    "\n",
    "    # Normalize the distances\n",
    "    pairwise_distances_norm = normalize(pairwise_distances)\n",
    "\n",
    "    # Compute persistence image\n",
    "    persistence_image = np.mean(pairwise_distances_norm, axis=0)\n",
    "    return persistence_image\n",
    "\n",
    "# Folder containing images\n",
    "folder_path = 'C:/Users/91703/Desktop/project/AD/ad1 and layers'\n",
    "\n",
    "# Load images from folder\n",
    "img_stack = np.array(load_images_from_folder(folder_path))\n",
    "\n",
    "# Compute persistence image\n",
    "persistence_image = compute_persistence_image(img_stack)\n",
    "\n",
    "# Plot persistence image\n",
    "plt.imshow(persistence_image, cmap='jet')\n",
    "plt.title('Persistence Image')\n",
    "plt.colorbar(label='Persistence')\n",
    "plt.xlabel('Pixel')\n",
    "plt.ylabel('Pixel')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming you have loaded your data into pandas DataFrame `df`\n",
    "df=pd.read_csv(\"C:\\Users\\91703\\Downloads\\T_Construction Entropy - Sheet1.csv\")\n",
    "y = df['class_label'] # assuming 'Label\n",
    "# Separate features (Betti numbers, entropies) and labels (class label)\n",
    "X = df[['V-Construction Betti', 'T-Construction Betti', \n",
    "        'V-Construction Entropy', 'T-Construction Entropy']]\n",
    "y = df['Class Label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   slno  h0_betti  h1_betti  h2_betti  h0_entropy  h1_entropy  h2_entropy  \\\n",
      "0     1      5452     15670      7988    3.531303    4.269367    4.389953   \n",
      "1     2      7362     23139     12761    3.750723    4.358696    4.373713   \n",
      "2     3      6000     17879      9169    3.770342    4.403850    4.499302   \n",
      "3     4      4896     18464     10295    4.158580    4.517729    4.436875   \n",
      "4     5      6374     19501     10570    3.760222    4.295249    4.338969   \n",
      "\n",
      "  class_label  \n",
      "0          CI  \n",
      "1          CI  \n",
      "2          CI  \n",
      "3          CI  \n",
      "4          CI  \n",
      "Accuracy: 66.66666666666666 %\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.57      1.00      0.73         4\n",
      "          CI       0.50      0.50      0.50         2\n",
      "          CN       1.00      0.50      0.67         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.69      0.67      0.63        12\n",
      "weighted avg       0.77      0.67      0.66        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df=pd.read_csv(\"C:/Users/91703/Downloads/T_Construction Entropy - Sheet1.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame to understand the structure and format\n",
    "print(df.head())\n",
    "\n",
    "# Separate features (Betti numbers, entropies) and labels (class label)\n",
    "X = df[['h0_betti', 'h1_betti', 'h2_betti', 'h0_entropy', 'h1_entropy', 'h2_entropy']]\n",
    "y = df['class_label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Initialize Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=2)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy*100,\"%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.25\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.00      0.00      0.00         5\n",
      "          CI       1.00      0.50      0.67         4\n",
      "          CN       0.12      0.33      0.18         3\n",
      "\n",
      "    accuracy                           0.25        12\n",
      "   macro avg       0.38      0.28      0.28        12\n",
      "weighted avg       0.36      0.25      0.27        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df=pd.read_csv(\"C:/Users/91703/Downloads/T_Construction Entropy - Sheet1.csv\")\n",
    "\n",
    "# Separate features (Betti numbers, entropies) and labels (class label)\n",
    "X = df[['h0_betti', 'h1_betti', 'h2_betti', 'h0_entropy', 'h1_entropy', 'h2_entropy']]\n",
    "y = df['class_label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AD       0.00      0.00      0.00         5\n",
      "          CI       0.60      0.75      0.67         4\n",
      "          CN       0.43      1.00      0.60         3\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.34      0.58      0.42        12\n",
      "weighted avg       0.31      0.50      0.37        12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91703\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\91703\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\91703\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\91703\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "df=pd.read_csv(\"C:/Users/91703/Downloads/T_Construction Entropy - Sheet1.csv\")\n",
    "\n",
    "\n",
    "# Separate features (Betti numbers, entropies) and labels (class label)\n",
    "X = df[['h0_betti', 'h1_betti', 'h2_betti', 'h0_entropy', 'h1_entropy', 'h2_entropy']]\n",
    "y = df['class_label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Logistic Regression classifier\n",
    "log_reg_classifier = LogisticRegression(C=1.0, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "log_reg_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = log_reg_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
